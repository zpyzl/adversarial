user10000613@jupyter-user10000613-2dserver4435:~/notespace/Chinese-Text-Classification-Pytorch$ python3 run.py --model TextCNN_PGD
Loading data...
Vocab size: 4762
180000it [00:03, 59085.10it/s]
10000it [00:00, 37476.68it/s]
10000it [00:00, 67550.16it/s]
Time usage: 0:00:03

<bound method Module.parameters of Model(
  (embedding): Embedding(4762, 300)
  (convs): ModuleList(
    (0): Conv2d(1, 256, kernel_size=(2, 300), stride=(1, 1))
    (1): Conv2d(1, 256, kernel_size=(3, 300), stride=(1, 1))
    (2): Conv2d(1, 256, kernel_size=(4, 300), stride=(1, 1))
  )
  (dropout): Dropout(p=0.5)
  (fc): Linear(in_features=768, out_features=10, bias=True)
)>
Epoch [1/20]
Iter:      0,  Train Loss:   2.3,  Train Acc:  9.38%,  Val Loss:   2.8,  Val Acc: 10.03%,  Time: 0:00:03 *
Iter:    100,  Train Loss:  0.69,  Train Acc: 73.44%,  Val Loss:  0.68,  Val Acc: 79.12%,  Time: 0:00:58 *
Iter:    200,  Train Loss:  0.66,  Train Acc: 76.56%,  Val Loss:  0.54,  Val Acc: 83.64%,  Time: 0:01:55 *
Iter:    300,  Train Loss:  0.41,  Train Acc: 86.72%,  Val Loss:  0.49,  Val Acc: 84.90%,  Time: 0:02:50 *

Iter:    400,  Train Loss:  0.74,  Train Acc: 80.47%,  Val Loss:  0.47,  Val Acc: 85.52%,  Time: 0:03:46 *
Iter:    500,  Train Loss:   0.4,  Train Acc: 88.28%,  Val Loss:  0.42,  Val Acc: 86.88%,  Time: 0:04:42 *
Iter:    600,  Train Loss:  0.49,  Train Acc: 83.59%,  Val Loss:  0.42,  Val Acc: 86.94%,  Time: 0:05:38 *
Iter:    700,  Train Loss:  0.42,  Train Acc: 86.72%,  Val Loss:   0.4,  Val Acc: 87.52%,  Time: 0:06:35 *
Iter:    800,  Train Loss:  0.48,  Train Acc: 86.72%,  Val Loss:  0.39,  Val Acc: 88.27%,  Time: 0:07:30 *
Iter:    900,  Train Loss:  0.41,  Train Acc: 90.62%,  Val Loss:  0.38,  Val Acc: 88.40%,  Time: 0:08:27 *
Iter:   1000,  Train Loss:   0.3,  Train Acc: 89.84%,  Val Loss:  0.38,  Val Acc: 88.32%,  Time: 0:09:22 *
Iter:   1100,  Train Loss:  0.36,  Train Acc: 92.19%,  Val Loss:  0.38,  Val Acc: 88.38%,  Time: 0:10:19
Iter:   1200,  Train Loss:  0.38,  Train Acc: 87.50%,  Val Loss:  0.36,  Val Acc: 89.24%,  Time: 0:11:14 *
Iter:   1300,  Train Loss:  0.43,  Train Acc: 84.38%,  Val Loss:  0.36,  Val Acc: 88.89%,  Time: 0:12:11
Iter:   1400,  Train Loss:  0.49,  Train Acc: 84.38%,  Val Loss:  0.35,  Val Acc: 89.27%,  Time: 0:13:06 *
Epoch [2/20]
Iter:   1500,  Train Loss:  0.42,  Train Acc: 86.72%,  Val Loss:  0.35,  Val Acc: 89.22%,  Time: 0:14:03
Iter:   1600,  Train Loss:   0.3,  Train Acc: 88.28%,  Val Loss:  0.35,  Val Acc: 89.28%,  Time: 0:14:58 *
Iter:   1700,  Train Loss:  0.36,  Train Acc: 90.62%,  Val Loss:  0.34,  Val Acc: 89.71%,  Time: 0:15:55 *
Iter:   1800,  Train Loss:  0.32,  Train Acc: 91.41%,  Val Loss:  0.35,  Val Acc: 89.41%,  Time: 0:16:50
Iter:   1900,  Train Loss:  0.35,  Train Acc: 89.84%,  Val Loss:  0.34,  Val Acc: 89.59%,  Time: 0:17:47
Iter:   2000,  Train Loss:  0.31,  Train Acc: 89.06%,  Val Loss:  0.34,  Val Acc: 89.46%,  Time: 0:18:29

Iter:   2100,  Train Loss:  0.32,  Train Acc: 92.19%,  Val Loss:  0.33,  Val Acc: 89.80%,  Time: 0:18:55 *
Iter:   2200,  Train Loss:   0.3,  Train Acc: 91.41%,  Val Loss:  0.33,  Val Acc: 89.71%,  Time: 0:19:22 *
Iter:   2300,  Train Loss:  0.29,  Train Acc: 92.97%,  Val Loss:  0.32,  Val Acc: 90.02%,  Time: 0:19:48 *
Iter:   2400,  Train Loss:   0.2,  Train Acc: 93.75%,  Val Loss:  0.33,  Val Acc: 89.96%,  Time: 0:20:14
Iter:   2500,  Train Loss:  0.15,  Train Acc: 95.31%,  Val Loss:  0.32,  Val Acc: 90.31%,  Time: 0:20:41 *

Iter:   2600,  Train Loss:  0.34,  Train Acc: 89.84%,  Val Loss:  0.32,  Val Acc: 90.30%,  Time: 0:21:07 *
Iter:   2700,  Train Loss:   0.2,  Train Acc: 90.62%,  Val Loss:  0.32,  Val Acc: 90.06%,  Time: 0:21:34
Iter:   2800,  Train Loss:   0.4,  Train Acc: 83.59%,  Val Loss:  0.32,  Val Acc: 90.11%,  Time: 0:22:00 *
Epoch [3/20]
Iter:   2900,  Train Loss:  0.31,  Train Acc: 92.19%,  Val Loss:  0.32,  Val Acc: 90.12%,  Time: 0:22:26
Iter:   3000,  Train Loss:  0.24,  Train Acc: 92.19%,  Val Loss:  0.32,  Val Acc: 89.97%,  Time: 0:22:53
Iter:   3100,  Train Loss:  0.24,  Train Acc: 92.19%,  Val Loss:  0.33,  Val Acc: 89.92%,  Time: 0:23:19
Iter:   3200,  Train Loss:  0.38,  Train Acc: 90.62%,  Val Loss:  0.33,  Val Acc: 89.98%,  Time: 0:23:45
Iter:   3300,  Train Loss:  0.31,  Train Acc: 88.28%,  Val Loss:  0.31,  Val Acc: 90.42%,  Time: 0:24:12 *
Iter:   3400,  Train Loss:  0.32,  Train Acc: 89.84%,  Val Loss:  0.32,  Val Acc: 90.12%,  Time: 0:24:38
Iter:   3500,  Train Loss:  0.17,  Train Acc: 95.31%,  Val Loss:  0.31,  Val Acc: 90.35%,  Time: 0:25:04
Iter:   3600,  Train Loss:  0.12,  Train Acc: 96.88%,  Val Loss:  0.31,  Val Acc: 90.16%,  Time: 0:25:31 *
Iter:   3700,  Train Loss:  0.35,  Train Acc: 86.72%,  Val Loss:  0.31,  Val Acc: 90.18%,  Time: 0:25:57
Iter:   3800,  Train Loss:  0.33,  Train Acc: 87.50%,  Val Loss:  0.32,  Val Acc: 90.16%,  Time: 0:26:23
Iter:   3900,  Train Loss:  0.26,  Train Acc: 92.97%,  Val Loss:  0.32,  Val Acc: 90.46%,  Time: 0:26:50
Iter:   4000,  Train Loss:  0.21,  Train Acc: 92.19%,  Val Loss:  0.32,  Val Acc: 90.38%,  Time: 0:27:17
Iter:   4100,  Train Loss:   0.3,  Train Acc: 89.06%,  Val Loss:  0.31,  Val Acc: 90.40%,  Time: 0:27:43
Iter:   4200,  Train Loss:  0.25,  Train Acc: 89.84%,  Val Loss:  0.32,  Val Acc: 89.85%,  Time: 0:28:09
Epoch [4/20]

Iter:   4300,  Train Loss:  0.17,  Train Acc: 93.75%,  Val Loss:  0.31,  Val Acc: 90.56%,  Time: 0:28:36 *
Iter:   4400,  Train Loss:  0.16,  Train Acc: 95.31%,  Val Loss:  0.31,  Val Acc: 90.52%,  Time: 0:29:02 *
Iter:   4500,  Train Loss:   0.3,  Train Acc: 92.97%,  Val Loss:  0.32,  Val Acc: 90.40%,  Time: 0:29:29
Iter:   4600,  Train Loss:   0.2,  Train Acc: 94.53%,  Val Loss:  0.31,  Val Acc: 90.48%,  Time: 0:29:55

Iter:   4700,  Train Loss:  0.32,  Train Acc: 91.41%,  Val Loss:  0.31,  Val Acc: 90.63%,  Time: 0:30:21 *
Iter:   4800,  Train Loss:  0.13,  Train Acc: 96.88%,  Val Loss:  0.31,  Val Acc: 90.60%,  Time: 0:30:48
Iter:   4900,  Train Loss:  0.15,  Train Acc: 95.31%,  Val Loss:  0.31,  Val Acc: 90.44%,  Time: 0:31:14
Iter:   5000,  Train Loss:  0.21,  Train Acc: 92.97%,  Val Loss:  0.31,  Val Acc: 90.45%,  Time: 0:31:40
Iter:   5100,  Train Loss:  0.24,  Train Acc: 91.41%,  Val Loss:  0.31,  Val Acc: 90.76%,  Time: 0:32:07 *
Iter:   5200,  Train Loss:  0.24,  Train Acc: 93.75%,  Val Loss:  0.31,  Val Acc: 90.72%,  Time: 0:32:33 *
Iter:   5300,  Train Loss:  0.17,  Train Acc: 92.97%,  Val Loss:  0.31,  Val Acc: 90.66%,  Time: 0:33:00
Iter:   5400,  Train Loss:  0.31,  Train Acc: 89.84%,  Val Loss:  0.32,  Val Acc: 90.36%,  Time: 0:33:26
Iter:   5500,  Train Loss:  0.23,  Train Acc: 93.75%,  Val Loss:  0.31,  Val Acc: 90.77%,  Time: 0:33:52
Iter:   5600,  Train Loss:   0.1,  Train Acc: 96.09%,  Val Loss:  0.31,  Val Acc: 90.33%,  Time: 0:34:19
Epoch [5/20]
Iter:   5700,  Train Loss:  0.25,  Train Acc: 92.97%,  Val Loss:  0.31,  Val Acc: 90.45%,  Time: 0:34:45
Iter:   5800,  Train Loss:  0.11,  Train Acc: 97.66%,  Val Loss:  0.32,  Val Acc: 90.62%,  Time: 0:35:11
Iter:   5900,  Train Loss:  0.12,  Train Acc: 96.88%,  Val Loss:  0.31,  Val Acc: 90.78%,  Time: 0:35:38
Iter:   6000,  Train Loss:  0.19,  Train Acc: 91.41%,  Val Loss:  0.31,  Val Acc: 90.84%,  Time: 0:36:04
Iter:   6100,  Train Loss:  0.24,  Train Acc: 92.19%,  Val Loss:  0.31,  Val Acc: 90.59%,  Time: 0:36:30
Iter:   6200,  Train Loss:   0.1,  Train Acc: 96.09%,  Val Loss:  0.32,  Val Acc: 90.33%,  Time: 0:36:57
No optimization for a long time, auto-stopping...
Test Loss:  0.29,  Test Acc: 91.11%
Precision, Recall and F1-Score...
               precision    recall  f1-score   support

      finance     0.9236    0.8820    0.9023      1000
       realty     0.9231    0.9360    0.9295      1000
       stocks     0.8654    0.8490    0.8571      1000
    education     0.9453    0.9500    0.9476      1000
      science     0.8693    0.8780    0.8736      1000
      society     0.8752    0.9260    0.8999      1000
     politics     0.8873    0.9050    0.8960      1000
       sports     0.9539    0.9520    0.9530      1000
         game     0.9571    0.8920    0.9234      1000
entertainment     0.9163    0.9410    0.9285      1000

     accuracy                         0.9111     10000
    macro avg     0.9116    0.9111    0.9111     10000
 weighted avg     0.9116    0.9111    0.9111     10000

Confusion Matrix...
[[882  18  54   4   7  17  10   4   2   2]
 [ 11 936  14   2   3  17   5   3   1   8]
 [ 46  24 849   6  34   5  29   2   3   2]
 [  0   4   1 950   3  20   8   4   1   9]
 [  3   6  29   5 878  17  25   3  21  13]
 [  1  16   1  17   5 926  21   1   2  10]
 [  8   6  22  12  14  27 905   2   0   4]
 [  0   1   1   1   3  10   6 952   3  23]
 [  3   0   9   5  53   8   5  10 892  15]
 [  1   3   1   3  10  11   6  17   7 941]]